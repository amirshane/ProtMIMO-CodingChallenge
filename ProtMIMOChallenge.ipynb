{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "vEVqiC8PkL2H",
        "MiB1v70_spE_",
        "jO4OxuyjP_Pe",
        "5-IjTEKfeIZP",
        "-kCxD6-thqAy",
        "Q588A5Ulugbq",
        "G7PVNCI-h8P0",
        "MnhLnI_Cdbk7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ProtMIMO-CodingChallenge\n",
        "\n",
        "This is a machine learning (for proteins) coding challenge.  It aims to assess general modeling skills as well as paper reading/implementation skills. It is intended to be done in PyTorch. \n"
      ],
      "metadata": {
        "id": "oaGq8zmlmcWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions"
      ],
      "metadata": {
        "id": "vEVqiC8PkL2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The task is to use the Multi-Input Multi-Output (MIMO) architecture from [Havasi et al.](https://arxiv.org/abs/2010.06610) for protein function prediction. This architecture attempts to replace traditional ensembles by taking in multiple inputs and predicting multiple outputs aiming to match a single distribution at each output head. For inference they input the same value multiple times and take the average of the multiple predictions made by the MIMO model. Check out the first three pages of their paper for more details.\n",
        "\n",
        "You will be predicting log fluoresence from the primary sequences of the proteins, which is in the column \"primary\".\n",
        "\n",
        "Some code for working with the data is provided in the \"Helper Functions\" section below, which has methods for downloading and loading the GFP (fluorescence) data from [Tasks Assessing Protein Embeddings (TAPE)](https://github.com/songlab-cal/tape) as Pandas DataFrames. We recommend you write clean code that is well-documented, organized, and utilizes helper functions. \n",
        "\n",
        "You are expected to do the following in 3 hours and you may use any resources available to you (except asking someone else for help or finding an existing solution):\n",
        "<ol>\n",
        "  <li> Read the first three pages of the MIMO paper (https://arxiv.org/abs/2010.06610). We strongly recommend doing this first. </1i>\n",
        "  <li> Exploratory data analysis. Observe the visualizations of the data provided and answer the questions at the bottom of the section. </li>\n",
        "  <li>Implement dataloaders and helper methods that enable you to train and evaluate the MIMO models.</li>\n",
        "  <li>Implement a MIMO CNN, a regression CNN, and an ensemble CNN.</li>\n",
        "  <li>Write a training loop and train each of these networks.</li>\n",
        "  <li>Run code to compare the performance of the MIMO model, the regression CNN, and the ensemble CNN on the following metrics: mean-squared error (MSE), Pearson correlation, and Spearman Rho. Answer the questions at the bottom of the section.</li>\n",
        "  <li>Estimate the amount of time you spent completing the challenge and include this value in your solution. If you end up needing more than 3 hours, that's okay, a completed solution that takes longer than expected is better than an incomplete solution, but please specify the total amount of time you spent.</li>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "IcjiZJK7kRwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs"
      ],
      "metadata": {
        "id": "MiB1v70_spE_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTFEipknhEQC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88251080-42e5-4b4b-9e72-6bb3d905be65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ProtMIMO-CodingChallenge' already exists and is not an empty directory.\n",
            "mv: cannot stat 'ProtMIMO-CodingChallenge/ProtMIMO/': No such file or directory\n",
            "mv: cannot stat 'ProtMIMO/fluorescence': No such file or directory\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tape-proteins==0.5 in /usr/local/lib/python3.9/dist-packages (0.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.9/dist-packages (from tape-proteins==0.5) (1.26.107)\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.9/dist-packages (from tape-proteins==0.5) (1.80)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.9/dist-packages (from tape-proteins==0.5) (2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from tape-proteins==0.5) (1.10.1)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.9/dist-packages (from tape-proteins==0.5) (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from tape-proteins==0.5) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from tape-proteins==0.5) (3.10.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from tape-proteins==0.5) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from biopython->tape-proteins==0.5) (1.22.4)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from boto3->tape-proteins==0.5) (0.6.0)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.107 in /usr/local/lib/python3.9/dist-packages (from boto3->tape-proteins==0.5) (1.29.107)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from boto3->tape-proteins==0.5) (1.0.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->tape-proteins==0.5) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->tape-proteins==0.5) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->tape-proteins==0.5) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->tape-proteins==0.5) (2022.12.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorboardX->tape-proteins==0.5) (23.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.8.0 in /usr/local/lib/python3.9/dist-packages (from tensorboardX->tape-proteins==0.5) (3.20.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.9/dist-packages (from botocore<1.30.0,>=1.29.107->boto3->tape-proteins==0.5) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.107->boto3->tape-proteins==0.5) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: biopython==1.80 in /usr/local/lib/python3.9/dist-packages (1.80)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from biopython==1.80) (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/amirshane/ProtMIMO-CodingChallenge.git\n",
        "!mv ProtMIMO-CodingChallenge/ProtMIMO/ ProtMIMO\n",
        "!mv ProtMIMO/fluorescence .\n",
        "!pip install tape-proteins==0.5\n",
        "!pip install biopython==1.80"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper functions"
      ],
      "metadata": {
        "id": "jO4OxuyjP_Pe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tape.datasets import LMDBDataset\n",
        "\n",
        "GFP_AMINO_ACID_VOCABULARY = [\"A\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\", \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"W\", \"Y\", \".\"]\n",
        "GFP_ALPHABET = {aa:i for i, aa in enumerate(GFP_AMINO_ACID_VOCABULARY)}\n",
        "\n",
        "def gfp_dataset_to_df(in_name):\n",
        "    \"\"\"Get the GFP dataset as a dataframe\"\"\"\n",
        "    dataset = LMDBDataset(in_name)\n",
        "    df = pd.DataFrame(list(dataset)[:])\n",
        "    df[\"log_fluorescence\"] = df.log_fluorescence.apply(lambda x: x[0])\n",
        "    return df\n",
        "\n",
        "def get_gfp_dfs():\n",
        "    \"\"\"Get train, val, and test dataframes for the gfp dataset\"\"\"\n",
        "    train_df = gfp_dataset_to_df(\"fluorescence/fluorescence_train.lmdb\")\n",
        "    val_df = gfp_dataset_to_df(\"fluorescence/fluorescence_valid.lmdb\")\n",
        "    test_df = gfp_dataset_to_df(\"fluorescence/fluorescence_test.lmdb\")\n",
        "    return train_df, val_df, test_df"
      ],
      "metadata": {
        "id": "K8UQMcH2VzQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Read MIMO paper"
      ],
      "metadata": {
        "id": "5-IjTEKfeIZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**To do:**\n",
        "- Read the first three pages of [the MIMO paper](https://arxiv.org/abs/2010.06610)"
      ],
      "metadata": {
        "id": "0uy2YcKMeMew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Exploratory data analysis"
      ],
      "metadata": {
        "id": "-kCxD6-thqAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Description**\n",
        "\n",
        "Here we load the data as Pandas DataFrames and investigate the columns \"primary\", \"log_fluorescence\", and \"num_mutations\". We've given you a brief summary of the data by creating histograms of the log fluorescence values and the number of mutations in both the train set and the test set. Do you notice anything interesting? You will be predicting log fluoresence from the primary sequences of the proteins, which is in the column \"primary\".\n",
        "\n",
        "**To do:**\n",
        "\n",
        "- Run the code and answer the questions at the end of this section"
      ],
      "metadata": {
        "id": "NyjzNb_zV3Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "train_df, val_df, test_df = get_gfp_dfs()\n",
        "print(len(train_df))\n",
        "print(len(val_df))\n",
        "print(len(test_df))\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "AmEo2SihhLbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "axes[0, 0].hist(train_df['num_mutations'])\n",
        "axes[0, 0].set_xlabel('num mutations train')\n",
        "axes[0, 0].set_ylabel('num instances')\n",
        "axes[0, 1].hist(train_df['log_fluorescence'])\n",
        "axes[0, 1].set_xlabel('log fluorescence train')\n",
        "axes[0, 1].set_ylabel('num instances')\n",
        "axes[1, 0].hist(test_df['num_mutations'])\n",
        "axes[1, 0].set_xlabel('num mutations test')\n",
        "axes[1, 0].set_ylabel('num instances')\n",
        "axes[1, 1].hist(test_df['log_fluorescence'])\n",
        "axes[1, 1].set_xlabel('log fluorescence test')\n",
        "axes[1, 1].set_ylabel('num instances')\n",
        "print()"
      ],
      "metadata": {
        "id": "bnwByOYRkc-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df['protein_length'].unique())\n",
        "print(test_df['protein_length'].unique())"
      ],
      "metadata": {
        "id": "DNzOAalNkjII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df[train_df['protein_length'] == 237]['primary'].iloc[0])\n",
        "print(train_df[train_df['protein_length'] == 236]['primary'].iloc[0])\n",
        "print(train_df[train_df['protein_length'] == 236]['primary'].iloc[1])\n",
        "print(train_df[train_df['protein_length'] == 235]['primary'].iloc[0])\n",
        "print(train_df[train_df['protein_length'] == 235]['primary'].iloc[1])"
      ],
      "metadata": {
        "id": "NTgtDrDrntij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions:\n",
        "1. What do you observe from the data visualizations shown? How do these observations affect your considerations in developing a machine learning model to predict log-fluorescence from primary sequence?\n",
        "2. From the sequence length data, you may notice that there are sequence deletions. Why might this be a problem when training a machine learning model? How might you handle these issues?\n",
        "3. Given this data, what neural network architectures would you consider for predicting log fluorescence from primary sequence? Explain their tradeoffs.\n"
      ],
      "metadata": {
        "id": "8GFDACoEr_eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Dataloading"
      ],
      "metadata": {
        "id": "Q588A5Ulugbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Pytorch Dataset to wrap the the pandas dataframe and do appropriate preprocessing. We've provided some functions that may be helpful in the \"Helper Functions\" section above.\n",
        "\n",
        "**To do:**\n",
        "- Implement a Pytorch Dataset"
      ],
      "metadata": {
        "id": "nEsD03_bXH9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class GFPDataset(Dataset):\n",
        "    pass"
      ],
      "metadata": {
        "id": "ZbTpNfopXNzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: MIMO CNN"
      ],
      "metadata": {
        "id": "G7PVNCI-h8P0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a MIMO CNN, a simple CNN regression network, and an ensemble of CNNs.\n",
        "\n",
        "**To do:**\n",
        "- Implement a MIMO CNN\n",
        "- Implement a CNN regression model\n",
        "- Implement a CNN ensemble model"
      ],
      "metadata": {
        "id": "SVEoRId5X6Xm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class MIMOCNN(nn.Module):\n",
        "    pass\n",
        "\n",
        "class EnsembleCNN(nn.Module):\n",
        "    pass\n",
        "\n",
        "class RegressionCNN(nn.Module):\n",
        "    pass"
      ],
      "metadata": {
        "id": "DVdabaEydrW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5: Training"
      ],
      "metadata": {
        "id": "bQvfzJeuiEw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a training loop and plot loss curves for the training of three models: a MIMO CNN, an ensemble of 3 CNNs, and a 3-input 3-output MIMO CNN. Make sure to use the provided `save_chkpt` and `train_driver` functions, as the analysis code provided in the next section relies on models being saved in the provided format.\n",
        "\n",
        "**To do:**\n",
        "- Write a training loop\n",
        "- Train a 3-input 3-output MIMO CNN model\n",
        "- Train a simple CNN regression network\n",
        "- Train a traditional ensemble model of 3 CNNs"
      ],
      "metadata": {
        "id": "8EhQwajEX9yN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from collections import deque\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "def save_chkpt(model_path, model, optimizer, epoch, batch, loss_domain, val_losses, train_losses):\n",
        "    \"\"\"Save a training checkpoint\n",
        "    Args:\n",
        "        model_path (str): the path to save the model to\n",
        "        model (nn.Module): the model to save\n",
        "        optimizer (torch.optim.Optimizer): the optimizer to save\n",
        "        epoch (int): the current epoch\n",
        "        batch (int): the current batch in the epoch\n",
        "        loss_domain (list of int): a list of the shared domain for val and training \n",
        "            losses\n",
        "        val_losses (list of float): a list containing the validation losses\n",
        "        train_losses (list of float): a list containing the training losses\n",
        "    \"\"\"\n",
        "    os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
        "    state_dict = dict()\n",
        "    state_dict.update({'model':model.state_dict(),\n",
        "                       'optimizer':optimizer.state_dict(),\n",
        "                       'epoch':epoch,\n",
        "                       'batch':batch,\n",
        "                       'loss_domain':loss_domain,\n",
        "                       'train_losses':train_losses,\n",
        "                       'val_losses':val_losses\n",
        "                       })\n",
        "    torch.save(state_dict, model_path)\n",
        "\n",
        "def train_driver(model_type):\n",
        "    \"\"\"Driver to set hyperparameters and train networks\"\"\"\n",
        "    num_ensemble = 3\n",
        "    hidden_dim = 50 # feel free to change\n",
        "    out_dim = 1\n",
        "    n_layers = 3 # feel free to change\n",
        "    n_epochs = 2 # feel free to change\n",
        "    device = 'cuda:0'\n",
        "    weight_decay = 1e-5 # feel free to change\n",
        "    vocab_size = len(GFP_AMINO_ACID_VOCABULARY)\n",
        "    seq_len = 237\n",
        "    batch_size = 36 # feel free to change\n",
        "\n",
        "    if model_type == 'mimo':\n",
        "        model_path = './models/mimo.pt'\n",
        "    elif model_type == 'ensemble':\n",
        "        model_path = './models/ensemble.pt'\n",
        "    elif model_type == 'regression':\n",
        "        model_path = './models/regression.pt'\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported model_type. Choose from ['mimo', 'ensemble', 'regression']\")\n",
        "\n",
        "    # Implement training"
      ],
      "metadata": {
        "id": "eDSGdzxAfJd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 6: Evaluation"
      ],
      "metadata": {
        "id": "76WrqYqbiR_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following cells to evaluate the models and compare their performance on the aforementioned metrics.\n",
        "\n",
        "**To do:**\n",
        "- Run the following cells and answer the questions at the bottom of this section."
      ],
      "metadata": {
        "id": "EqLIWU_MeolI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import mean_squared_error \n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "def load_model(model_path, model_class):\n",
        "    \"\"\"Load a saved model\"\"\"\n",
        "    num_ensemble = 3\n",
        "    hidden_dim = 50 # feel free to change\n",
        "    out_dim = 1\n",
        "    n_layers = 3 # feel free to change\n",
        "    n_epochs = 2 # feel free to change\n",
        "    device = 'cuda:0'\n",
        "    vocab_size = len(GFP_AMINO_ACID_VOCABULARY)\n",
        "    batch_size = 36 # feel free to change\n",
        "    seq_len = 237\n",
        "\n",
        "    model = model_class(num_ensemble=num_ensemble, hidden_dim=hidden_dim, out_dim=out_dim, vocab_size=vocab_size, n_layers=n_layers, seq_len=seq_len)\n",
        "    model.load_state_dict(torch.load(model_path)['model'])\n",
        "    model.to(device)\n",
        "    return model\n",
        "\n",
        "def get_preds_targs(model, dataloader):\n",
        "    \"\"\"Get predictions for a model and ground truth values from a dataset\"\"\"\n",
        "    device = next(model.parameters())\n",
        "    preds = []\n",
        "    targs = []\n",
        "    with torch.inference_mode():\n",
        "        model.eval()\n",
        "        with tqdm(total=len(dataloader)) as pbar:\n",
        "            for x, y in dataloader:\n",
        "                x = x.to(device)\n",
        "                pred = model(x)\n",
        "                preds.append(pred.detach().cpu().numpy())\n",
        "                targs.append(y.detach().numpy())\n",
        "                pbar.update(1)\n",
        "\n",
        "    preds = np.array(preds).squeeze()\n",
        "    targs = np.array(targs).squeeze()\n",
        "    return preds, targs\n",
        "\n",
        "def get_stats(pred, targ):\n",
        "    mse = mean_squared_error(targ, pred)\n",
        "    corr = pearsonr(targ, pred)[0]\n",
        "    rank_corr = spearmanr(targ, pred)[0]\n",
        "    return mse, corr, rank_corr\n",
        "\n",
        "def print_stats(pred, targ):\n",
        "    mse, corr, rank_corr = get_stats(pred, targ)\n",
        "    print(\"Stats:\")\n",
        "    print(\"MSE: \" + str(mse))\n",
        "    print(\"Pearson correlation: \", str(corr))\n",
        "    print(\"Spearman rank correlation: \", str(rank_corr))\n",
        "\n",
        "def test_models():\n",
        "    \"\"\"Evaluate trained models according to several metrics\"\"\"\n",
        "    model_names = ['mimo', 'ensemble', 'regression']\n",
        "    model_paths = ['./models/mimo.pt', './models/ensemble.pt', './models/regression.pt']\n",
        "    model_classes = [MIMOCNN, EnsembleCNN, RegressionCNN]\n",
        "\n",
        "    _, _, test_df = get_gfp_dfs()\n",
        "    test_dataset = GFPDataset(test_df)\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(model_names), figsize=(15, 5))\n",
        "    for i, (model_name, model_path, model_class) in enumerate(zip(model_names, model_paths, model_classes)):\n",
        "        model = load_model(model_path, model_class)\n",
        "        preds, targs = get_preds_targs(model, test_loader)\n",
        "        print(f\"Model: {model_name}\")\n",
        "        print_stats(pred=preds, targ=targs)\n",
        "        axes[i].scatter(preds, targs)\n",
        "        axes[i].set_xlabel('prediction')\n",
        "        axes[i].set_ylabel('true value')\n",
        "        axes[i].set_title(model_name)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "N4lk_hDMiUh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_models()"
      ],
      "metadata": {
        "id": "lzIw631PKWAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Questions:\n",
        "- How did you expect the three models to perform relative to each other? Did empirical performance meet your expectations? If not, explain why you think that might be."
      ],
      "metadata": {
        "id": "WbrJx29ShHQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 7: Time spent"
      ],
      "metadata": {
        "id": "MnhLnI_Cdbk7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Todo:**\n",
        "- Record the amount of time spent on this challenge"
      ],
      "metadata": {
        "id": "kvbnclWaddG-"
      }
    }
  ]
}
